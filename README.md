# ğŸ“„ AI Document Summarizer & Q/A System â€” RAG Pipeline (Pinecone + Groq)

A production-ready Retrieval-Augmented Generation (RAG) system capable of:

* Ingesting **single or multiple PDFs**
* Splitting them into high-quality overlapping chunks
* Embedding and storing vectors in **Pinecone**
* Answering questions with **Groq-powered Llama 3.3**
* Generating **short, medium, and detailed summaries**
* Producing **clean, formatted responses** for API/UI integration

This project demonstrates a complete, modular, DVC-tracked pipeline suitable for **enterprise document automation**, **research assistants**, or **intelligent PDF reading agents**.

---

## âœ¨ Features

* ğŸ“š **Multi-Document Loader** (PDF â†’ text)
* ğŸ”ª **Recursive Text Splitter** with configurable chunking
* ğŸ§  **Embedding Generator** using free HuggingFace MiniLM (384 dims)
* ğŸ“¦ **Vector Storage** in Pinecone with serverless spec
* ğŸ” **Retriever** using semantic similarity search
* ğŸ¤– **Groq LLM** for ultra-fast inference (Llama 3.3)
* ğŸ“ **Three-level Summary Generator**

  * TL;DR
  * Structured
  * Detailed multi-section
* ğŸ§¼ **Output Formatter** for clean responses
* ğŸ”— **Full RAG Pipeline** combining retrieval + generation
* âš™ï¸ **Modular Architecture** ready for FastAPI deployment
* ğŸ“ **DVC Integration** for reproducible stages

---

## ğŸ› ï¸ Tech Stack

| Component         | Technology                           |
| ----------------- | ------------------------------------ |
| Embeddings        | all-MiniLM-L6-v2 (HuggingFace)       |
| Vector Store      | Pinecone (serverless)                |
| LLM               | Groq â€” Llama 3.3                     |
| Pipeline Tracking | DVC                                  |
| Runtime           | Python 3.10                          |
| Orchestration     | Modular component-based architecture |
| UI (Upcoming)     | FastAPI + Next.js                    |

---

## ğŸ“‚ Project Structure

```
AI_Document_Summarizer_QA_System/
â”œâ”€â”€ Data/
â”‚   â””â”€â”€ <your PDFs>
â”œâ”€â”€ research/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ AI_Document_Summarizer_QA_System/
â”‚       â”œâ”€â”€ components/
â”‚       â”‚   â”œâ”€â”€ stage_01_document_loader.py
â”‚       â”‚   â”œâ”€â”€ stage_02_text_splitter.py
â”‚       â”‚   â”œâ”€â”€ stage_03_store_index.py
â”‚       â”‚   â”œâ”€â”€ stage_04_RAG.py
â”‚       â”‚   â””â”€â”€ stage_05_Output.py
â”‚       â”œâ”€â”€ logging/
â”‚       â”œâ”€â”€ utils/
â”œâ”€â”€ templates/
â”œâ”€â”€ .env
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ dvc.yaml
â”œâ”€â”€ LICENSE
â”œâ”€â”€ main.py
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ template.py
```

---

## ğŸ“¦ Installation & Setup

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/Priyanshu1303d/AI_Document_Summarizer_QA_System.git
cd AI_Document_Summarizer_QA_System
```

### 2ï¸âƒ£ Create and activate a virtual environment

```bash
python -m venv .venv
# Windows
.venv\Scripts\activate
# Linux/Mac
source .venv/bin/activate
```

### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Configure your `.env`

```env
PINECONE_API_KEY=your_key
PINECONE_REGION=us-east-1
PINECONE_INDEX_NAME=ai-document-index

GROQ_API_KEY=your_groq_key
```

### (Or You can simply copy and paste from .env.example to .env and add your api keys in it)

(HuggingFace MiniLM embeddings do **not** require any API key.)

---

## ğŸ”€ Pipeline Breakdown (DVC Stages)

| Stage | Component                     | Purpose                               |
| ----- | ----------------------------- | ------------------------------------- |
| 01    | `stage_01_document_loader.py` | Load PDFs into clean text             |
| 02    | `stage_02_text_splitter.py`   | Convert text â†’ overlapping chunks     |
| 03    | `stage_03_store_index.py`     | Embed + store chunks in Pinecone      |
| 04    | `stage_04_RAG.py`             | Retrieve â†’ LLM generate answer        |
| 05    | `stage_05_Output.py`          | Generate summaries + clean formatting |

---

## ğŸš€ Usage

### ğŸ‘‰ Run the full pipeline

```bash
dvc repro
```

### ğŸ‘‰ Test RAG and Summaries in `trial.ipynb`

You can run:

* Short / medium / detailed summaries
* Custom QA
* Retrieval checks
* Multi-file ingestion

Example:

```python
rag = RAGPipeline()
rag.ask("What is the third project described in the document?")
```

Generate summaries:

```python
summaries = summarizer.generate_all(text)
```

---

## ğŸ§ª Example Query

**User Question:**

> â€œWhat is the third capstone project mentioned in the document?â€

**Model Answer:**

> "The third capstone project described in the document is the **AI Image Generator**, which generates images using Stable Diffusion or DALLÂ·Eâ€¦"

**Retrieved Sources:**
(Shortened and cleaned automatically)

---

## ğŸŒŸ Why This Project Matters

This system demonstrates the **exact skills required for modern AI engineering roles**:

* Building RAG pipelines end-to-end
* Managing embeddings, vector stores, and retrieval
* Using fast open-weight LLMs (Groq)
* Modular backend engineering
* Clean architecture + reproducibility (DVC)
* Preparing for production deployment (FastAPI + Docker)

Perfect for roles involving:

âœ” AI automation
âœ” Document intelligence
âœ” LLM engineering
âœ” Backend ML systems

---

## ğŸ¤ Contributing

Feel free to open issues or submit PRs. Clean, modular code is always welcome.

---

## ğŸ“„ License

This project is licensed under the **MIT License**.

---

## âœï¸ Author

**Priyanshu Kumar Singh**
â­ GitHub: [https://github.com/Priyanshu1303d](https://github.com/Priyanshu1303d)

---

